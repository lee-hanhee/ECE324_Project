{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wavenet Class\n",
    "\n",
    "used chat for some of these - credit will come later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import librosa\n",
    "\n",
    "\n",
    "class WaveUNet(nn.Module):\n",
    "    def __init__(self, input_channels=1, output_channels=1, num_layers=6, features=24):\n",
    "        super(WaveUNet, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoders = nn.ModuleList([\n",
    "            nn.Conv1d(input_channels if i == 0 else features * (2**i), \n",
    "                      features * (2**(i+1)), kernel_size=15, stride=2, padding=7)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Decoder\n",
    "        self.decoders = nn.ModuleList([\n",
    "            nn.ConvTranspose1d(features * (2**(i+1)), \n",
    "                               features * (2**i), kernel_size=15, stride=2, padding=7, output_padding=1)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Final output layer\n",
    "        self.output_layer = nn.Conv1d(features, output_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_outs = []\n",
    "\n",
    "        # Encoder Pass\n",
    "        for encoder in self.encoders:\n",
    "            x = F.relu(encoder(x))\n",
    "            enc_outs.append(x)\n",
    "            print(f\"Encoder output shape: {x.shape}\")  # Debugging shape\n",
    "\n",
    "        # Decoder Pass\n",
    "        for i, decoder in enumerate(self.decoders):\n",
    "            print(f\"Decoder input shape before skip connection: {x.shape}\")\n",
    "            print(f\"Before decoding, input shape: {x.shape}\")\n",
    "            x = decoder(x)\n",
    "            print(f\"After decoding, output shape: {x.shape}\")\n",
    "            x = F.relu(x)\n",
    "            print(f\"Decoder output shape: {x.shape}\")\n",
    "            if x.shape != enc_outs[-(i+1)].shape:\n",
    "                # Adjust the size using interpolate if necessary\n",
    "                print(\"hello\")\n",
    "                x = F.interpolate(x, size=enc_outs[-(i+1)].shape[2], mode='linear', align_corners=False)\n",
    "            \n",
    "            # Ensure decoder output matches encoder output size\n",
    "            x += enc_outs[-(i+1)]  # Skip connection\n",
    "            print(f\"Shape after skip connection: {x.shape}\")\n",
    "\n",
    "        # Final Output\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "@dataclass\n",
    "class AudioPair:\n",
    "    mixed_waveform: torch.Tensor\n",
    "    target_waveforms: torch.Tensor  # Multiple stems\n",
    "\n",
    "class SourceSeparationDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.track_folders = sorted(os.listdir(root_dir))  # List of tracks (Track00001, Track00002, ...)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.track_folders)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        track_folder = self.track_folders[idx]\n",
    "        track_path = os.path.join(self.root_dir, track_folder)\n",
    "\n",
    "        # Paths to mix and stems\n",
    "        mix_path = os.path.join(track_path, \"mix.wav\")\n",
    "        stems_path = os.path.join(track_path, \"stems\")  \n",
    "\n",
    "        # Load mixed waveform\n",
    "        _, mixed_waveform = wavfile.read(mix_path)\n",
    "        mixed_waveform = mixed_waveform.astype(np.float32) / 32768.0\n",
    "\n",
    "        # Load all stem waveforms\n",
    "        stem_files = sorted([f for f in os.listdir(stems_path) if f.endswith(\".wav\")])  \n",
    "        target_waveforms = []\n",
    "        \n",
    "        for stem_file in stem_files:\n",
    "            stem_path = os.path.join(stems_path, stem_file)\n",
    "            _, stem_waveform = wavfile.read(stem_path)\n",
    "            target_waveforms.append(stem_waveform.astype(np.float32) / 32768.0)\n",
    "\n",
    "        # Stack stems into (num_instruments, time)\n",
    "        target_waveforms = np.stack(target_waveforms)\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        mixed_waveform = torch.tensor(mixed_waveform, dtype=torch.float32).unsqueeze(0)  # (1, time)\n",
    "        target_waveforms = torch.tensor(target_waveforms, dtype=torch.float32)  # (num_instruments, time)\n",
    "\n",
    "        return AudioPair(mixed_waveform=mixed_waveform, target_waveforms=target_waveforms)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleTrackDataset(Dataset):\n",
    "    def __init__(self, track_path):\n",
    "        self.mix_path = os.path.join(track_path, \"mix.wav\")\n",
    "        self.stem_paths = sorted([\n",
    "            os.path.join(track_path, \"stems\", f) \n",
    "            for f in os.listdir(os.path.join(track_path, \"stems\")) if f.endswith(\".wav\")\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 1  # Only one track\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load mix\n",
    "        _, mixed_waveform = wavfile.read(self.mix_path)\n",
    "        mixed_waveform = mixed_waveform.astype(np.float32) / 32768.0\n",
    "        mixed_waveform = torch.tensor(mixed_waveform, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        # Load all stems\n",
    "        target_waveforms = []\n",
    "        for stem_path in self.stem_paths:\n",
    "            _, stem_waveform = wavfile.read(stem_path)\n",
    "            stem_waveform = stem_waveform.astype(np.float32) / 32768.0\n",
    "            target_waveforms.append(torch.tensor(stem_waveform, dtype=torch.float32))\n",
    "\n",
    "        target_waveforms = torch.stack(target_waveforms)  # Shape: [num_stems, time]\n",
    "\n",
    "        return AudioPair(mixed_waveform=mixed_waveform, target_waveforms=target_waveforms)  # Fixed the argument name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 50\n",
    "batch_size = 8\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = WaveUNet(input_channels=1, output_channels=10).to(device)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for waveform reconstruction\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(mixed, target):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(mixed)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(output, target)\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "def separate_audio(model, mixed_audio):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mixed_audio = torch.tensor(mixed_audio).unsqueeze(0).unsqueeze(0)  # Add batch & channel dims\n",
    "        separated = model(mixed_audio)\n",
    "    return separated.squeeze(0).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loader coallate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_pair_collate(batch):\n",
    "    mixed_waveforms = [item.mixed_waveform for item in batch]\n",
    "    target_waveforms = [item.target_waveforms for item in batch]\n",
    "    \n",
    "    # Stack the tensors for each field\n",
    "    mixed_waveforms = torch.stack(mixed_waveforms)\n",
    "    target_waveforms = torch.stack(target_waveforms)\n",
    "    \n",
    "    return AudioPair(mixed_waveform=mixed_waveforms, target_waveforms=target_waveforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: torch.Size([1, 48, 1932458])\n",
      "Encoder output shape: torch.Size([1, 96, 966229])\n",
      "Encoder output shape: torch.Size([1, 192, 483115])\n",
      "Encoder output shape: torch.Size([1, 384, 241558])\n",
      "Encoder output shape: torch.Size([1, 768, 120779])\n",
      "Encoder output shape: torch.Size([1, 1536, 60390])\n",
      "Decoder input shape before skip connection: torch.Size([1, 1536, 60390])\n",
      "Before decoding, input shape: torch.Size([1, 1536, 60390])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given transposed=1, weight of size [48, 24, 15], expected input[1, 1536, 60390] to have 48 channels, but got 1536 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m mixed, target \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mmixed_waveform, batch\u001b[38;5;241m.\u001b[39mtarget_waveforms\n\u001b[0;32m     24\u001b[0m mixed, target \u001b[38;5;241m=\u001b[39m mixed\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 25\u001b[0m loss \u001b[38;5;241m=\u001b[39m train_step(mixed, target)\n\u001b[0;32m     26\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[19], line 6\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(mixed, target)\u001b[0m\n\u001b[0;32m      3\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m output \u001b[38;5;241m=\u001b[39m model(mixed)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n",
      "File \u001b[1;32mc:\\Users\\Daphne\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Daphne\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[37], line 45\u001b[0m, in \u001b[0;36mWaveUNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(x)\u001b[38;5;241m.\u001b[39many() \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(x)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput tensor contains NaN or Inf values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 45\u001b[0m x \u001b[38;5;241m=\u001b[39m decoder(x)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter decoding, output shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[1;32mc:\\Users\\Daphne\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Daphne\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Daphne\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:974\u001b[0m, in \u001b[0;36mConvTranspose1d.forward\u001b[1;34m(self, input, output_size)\u001b[0m\n\u001b[0;32m    964\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    965\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    967\u001b[0m     output_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    972\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    973\u001b[0m )\n\u001b[1;32m--> 974\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv_transpose1d(\n\u001b[0;32m    975\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    978\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    979\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding,\n\u001b[0;32m    980\u001b[0m     output_padding,\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    982\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation,\n\u001b[0;32m    983\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given transposed=1, weight of size [48, 24, 15], expected input[1, 1536, 60390] to have 48 channels, but got 1536 channels instead"
     ]
    }
   ],
   "source": [
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "\n",
    "# Construct the correct path\n",
    "track_name = \"Track00001\"\n",
    "\n",
    "track_path = os.path.join(project_root, \"data\", \"raw\", track_name)\n",
    "\n",
    "dataset = SingleTrackDataset(track_path)\n",
    "train_loader = DataLoader(dataset, batch_size=100, shuffle=False, collate_fn=audio_pair_collate)\n",
    "\n",
    "# Initialize model\n",
    "num_stems = len(dataset[0].target_waveforms)  # Get number of instruments\n",
    "model = WaveUNet(input_channels=1, output_channels=num_stems)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        # Since batch is an AudioPair, access the attributes directly\n",
    "        mixed, target = batch.mixed_waveform, batch.target_waveforms\n",
    "        \n",
    "        mixed, target = mixed.to(device), target.to(device)\n",
    "        loss = train_step(mixed, target)\n",
    "        total_loss += loss.item()\n",
    "        print(f\"Batch, Loss: {total_loss:.6f}\")\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.6f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
