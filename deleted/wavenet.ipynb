{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wavenet Class\n",
    "\n",
    "used chat for some of these - credit will come later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.io import wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveUNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels: int = 1,\n",
    "        output_channels: int = 1,\n",
    "        num_layers: int = 6,\n",
    "        features: int = 24\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the Wave-U-Net model for end-to-end audio source separation.\n",
    "\n",
    "        Parameters:\n",
    "            input_channels (int): Number of input channels (e.g., 1 for mono audio).\n",
    "            output_channels (int): Number of output channels (e.g., 1 per source).\n",
    "            num_layers (int): Depth of the encoder/decoder layers.\n",
    "            features (int): Base number of feature maps in the first conv layer.\n",
    "        \"\"\"\n",
    "        super(WaveUNet, self).__init__()\n",
    "\n",
    "        # Encoder layers: Downsample with increasing feature maps\n",
    "        self.encoders = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=input_channels if i == 0 else features * (2 ** i),\n",
    "                out_channels=features * (2 ** (i + 1)),\n",
    "                kernel_size=15,\n",
    "                stride=2,\n",
    "                padding=7\n",
    "            )\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Decoder layers: Upsample with skip connections\n",
    "        self.decoders = nn.ModuleList([\n",
    "            nn.ConvTranspose1d(\n",
    "                in_channels=features * (2 ** (i + 1)),\n",
    "                out_channels=features * (2 ** i),\n",
    "                kernel_size=15,\n",
    "                stride=2,\n",
    "                padding=7,\n",
    "                output_padding=1\n",
    "            )\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Final output layer to match desired output channels\n",
    "        self.output_layer = nn.Conv1d(\n",
    "            in_channels=features,\n",
    "            out_channels=output_channels,\n",
    "            kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the Wave-U-Net.\n",
    "\n",
    "        Parameters:\n",
    "            x (Tensor): Input tensor of shape (batch, channels, time).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch, output_channels, time).\n",
    "        \"\"\"\n",
    "        enc_outs = []\n",
    "\n",
    "        # Encoder path with ReLU and storing outputs for skip connections\n",
    "        for encoder in self.encoders:\n",
    "            x = F.relu(encoder(x))\n",
    "            enc_outs.append(x)\n",
    "\n",
    "        # Decoder path with skip connections\n",
    "        for i, decoder in enumerate(self.decoders):\n",
    "            x = decoder(x)\n",
    "            x = F.relu(x)\n",
    "\n",
    "            skip_out = enc_outs[-(i + 1)]\n",
    "\n",
    "            # Match the length using interpolation if needed\n",
    "            if x.shape[2] != skip_out.shape[2]:\n",
    "                x = F.interpolate(\n",
    "                    x,\n",
    "                    size=skip_out.shape[2],\n",
    "                    mode='linear',\n",
    "                    align_corners=False\n",
    "                )\n",
    "\n",
    "            # Skip connection\n",
    "            x = x + skip_out\n",
    "\n",
    "        return self.output_layer(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AudioPair:\n",
    "    \"\"\"\n",
    "    A container for a mixed waveform and its corresponding target stems.\n",
    "\n",
    "    Attributes:\n",
    "        mixed_waveform (Tensor): Tensor of shape (1, time).\n",
    "        target_waveforms (Tensor): Tensor of shape (num_stems, time).\n",
    "    \"\"\"\n",
    "    mixed_waveform: torch.Tensor\n",
    "    target_waveforms: torch.Tensor\n",
    "\n",
    "\n",
    "class SourceSeparationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch dataset for loading source separation audio pairs.\n",
    "\n",
    "    Assumes each track directory contains:\n",
    "        - 'mix.wav' (the full mixture)\n",
    "        - 'stems/' folder with multiple stem .wav files\n",
    "\n",
    "    Example directory structure:\n",
    "        root_dir/\n",
    "            Track00001/\n",
    "                mix.wav\n",
    "                stems/\n",
    "                    S01.wav\n",
    "                    S02.wav\n",
    "                    ...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir: str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the dataset with the root directory containing track folders.\n",
    "\n",
    "        Parameters:\n",
    "            root_dir (str): Path to the dataset root directory.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.track_folders: List[str] = sorted(os.listdir(root_dir))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the number of tracks.\"\"\"\n",
    "        return len(self.track_folders)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> AudioPair:\n",
    "        \"\"\"\n",
    "        Loads the mixed waveform and its stem waveforms as tensors.\n",
    "\n",
    "        Parameters:\n",
    "            idx (int): Index of the track.\n",
    "\n",
    "        Returns:\n",
    "            AudioPair: A dataclass containing the mix and stem tensors.\n",
    "        \"\"\"\n",
    "        track_folder = self.track_folders[idx]\n",
    "        track_path = os.path.join(self.root_dir, track_folder)\n",
    "\n",
    "        # Load mixed waveform\n",
    "        mix_path = os.path.join(track_path, \"mix.wav\")\n",
    "        _, mixed_waveform = wavfile.read(mix_path)\n",
    "        mixed_waveform = mixed_waveform.astype(np.float32) / 32768.0\n",
    "\n",
    "        # Load stem waveforms\n",
    "        stems_path = os.path.join(track_path, \"stems\")\n",
    "        stem_files = sorted([\n",
    "            f for f in os.listdir(stems_path) if f.endswith(\".wav\")\n",
    "        ])\n",
    "\n",
    "        target_waveforms = []\n",
    "        for stem_file in stem_files:\n",
    "            stem_path = os.path.join(stems_path, stem_file)\n",
    "            _, stem_waveform = wavfile.read(stem_path)\n",
    "            stem_waveform = stem_waveform.astype(np.float32) / 32768.0\n",
    "            target_waveforms.append(stem_waveform)\n",
    "\n",
    "        # Stack stems: (num_stems, time)\n",
    "        target_waveforms = np.stack(target_waveforms)\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        mixed_tensor = torch.tensor(mixed_waveform, dtype=torch.float32).unsqueeze(0)  # (1, time)\n",
    "        stems_tensor = torch.tensor(target_waveforms, dtype=torch.float32)             # (num_stems, time)\n",
    "\n",
    "        return AudioPair(mixed_waveform=mixed_tensor, target_waveforms=stems_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class AudioPair:\n",
    "    \"\"\"\n",
    "    A container for a mixed waveform and its corresponding target stems.\n",
    "\n",
    "    Attributes:\n",
    "        mixed_waveform (Tensor): Tensor of shape (1, time).\n",
    "        target_waveforms (Tensor): Tensor of shape (num_stems, time).\n",
    "    \"\"\"\n",
    "    mixed_waveform: torch.Tensor\n",
    "    target_waveforms: torch.Tensor\n",
    "\n",
    "\n",
    "class SingleTrackDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset wrapper for a single track with one mix and multiple stems.\n",
    "\n",
    "    This is useful for inference or testing on one audio mixture.\n",
    "\n",
    "    Directory structure:\n",
    "        track_path/\n",
    "            mix.wav\n",
    "            stems/\n",
    "                S01.wav\n",
    "                S02.wav\n",
    "                ...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, track_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize with path to a single track.\n",
    "\n",
    "        Parameters:\n",
    "            track_path (str): Path to the directory containing mix.wav and stems/\n",
    "        \"\"\"\n",
    "        self.mix_path: str = os.path.join(track_path, \"mix.wav\")\n",
    "        stems_dir = os.path.join(track_path, \"stems\")\n",
    "\n",
    "        self.stem_paths: List[str] = sorted([\n",
    "            os.path.join(stems_dir, f)\n",
    "            for f in os.listdir(stems_dir)\n",
    "            if f.endswith(\".wav\")\n",
    "        ])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Always returns 1, since this dataset only wraps a single track.\n",
    "\n",
    "        Returns:\n",
    "            int: 1\n",
    "        \"\"\"\n",
    "        return 1\n",
    "\n",
    "    def __getitem__(self, idx: int) -> AudioPair:\n",
    "        \"\"\"\n",
    "        Loads and returns the mix and stem waveforms as tensors.\n",
    "\n",
    "        Parameters:\n",
    "            idx (int): Ignored, always returns the single track.\n",
    "\n",
    "        Returns:\n",
    "            AudioPair: A dataclass containing the mix and stem tensors.\n",
    "        \"\"\"\n",
    "        # Load mix waveform\n",
    "        _, mixed_waveform = wavfile.read(self.mix_path)\n",
    "        mixed_waveform = mixed_waveform.astype(np.float32) / 32768.0\n",
    "        mixed_tensor = torch.tensor(mixed_waveform, dtype=torch.float32).unsqueeze(0)  # (1, time)\n",
    "\n",
    "        # Load stem waveforms\n",
    "        target_waveforms = []\n",
    "        for stem_path in self.stem_paths:\n",
    "            _, stem_waveform = wavfile.read(stem_path)\n",
    "            stem_waveform = stem_waveform.astype(np.float32) / 32768.0\n",
    "            target_waveforms.append(torch.tensor(stem_waveform, dtype=torch.float32))\n",
    "\n",
    "        stems_tensor = torch.stack(target_waveforms)  # (num_stems, time)\n",
    "\n",
    "        return AudioPair(mixed_waveform=mixed_tensor, target_waveforms=stems_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Hyperparameters ===\n",
    "num_epochs: int = 50\n",
    "batch_size: int = 8\n",
    "learning_rate: float = 1e-3\n",
    "\n",
    "# === Device Setup ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Model, Loss, and Optimizer ===\n",
    "model = WaveUNet(input_channels=1, output_channels=10).to(device)\n",
    "\n",
    "# Mean Squared Error loss for waveform reconstruction\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Adam optimizer with specified learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_step(mixed: Tensor, target: Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Perform one training step on a batch of input/output waveforms.\n",
    "\n",
    "    Parameters:\n",
    "        mixed (Tensor): Input mixed waveform of shape (batch, 1, time).\n",
    "        target (Tensor): Ground truth stem waveforms of shape (batch, num_stems, time).\n",
    "\n",
    "    Returns:\n",
    "        float: The scalar loss value for this step.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(mixed)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(output, target)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def separate_audio(model: torch.nn.Module, mixed_audio: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Run source separation inference on a single input waveform.\n",
    "\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): Trained WaveUNet model.\n",
    "        mixed_audio (np.ndarray): Input mono waveform of shape (time,).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Output separated waveform(s) of shape (num_stems, time).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Prepare input tensor: (1, 1, time)\n",
    "        mixed_tensor = torch.tensor(mixed_audio, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "        separated = model(mixed_tensor)\n",
    "\n",
    "    # Remove batch dimension and convert to numpy: (num_stems, time)\n",
    "    return separated.squeeze(0).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loader coallate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def audio_pair_collate(batch: List[AudioPair]) -> AudioPair:\n",
    "    \"\"\"\n",
    "    Collate function for batching AudioPair samples from a DataLoader.\n",
    "\n",
    "    Stacks mixed_waveforms and target_waveforms into batched tensors.\n",
    "\n",
    "    Parameters:\n",
    "        batch (List[AudioPair]): List of AudioPair instances from the dataset.\n",
    "\n",
    "    Returns:\n",
    "        AudioPair: A new AudioPair with stacked tensors:\n",
    "                   - mixed_waveform: (batch_size, 1, time)\n",
    "                   - target_waveforms: (batch_size, num_stems, time)\n",
    "    \"\"\"\n",
    "    mixed_waveforms = [item.mixed_waveform for item in batch]   # List[(1, time)]\n",
    "    target_waveforms = [item.target_waveforms for item in batch]  # List[(num_stems, time)]\n",
    "\n",
    "    # Stack along batch dimension\n",
    "    mixed_batch = torch.stack(mixed_waveforms)     # (batch_size, 1, time)\n",
    "    targets_batch = torch.stack(target_waveforms)  # (batch_size, num_stems, time)\n",
    "\n",
    "    return AudioPair(mixed_waveform=mixed_batch, target_waveforms=targets_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given transposed=1, weight of size [48, 24, 15], expected input[1, 1536, 60390] to have 48 channels, but got 1536 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m target \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mtarget_waveforms\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Train step\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmixed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(mixed, target)\u001b[0m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmixed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[2], line 71\u001b[0m, in \u001b[0;36mWaveUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Decoder path with skip connections\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, decoder \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoders):\n\u001b[0;32m---> 71\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     74\u001b[0m     skip_out \u001b[38;5;241m=\u001b[39m enc_outs[\u001b[38;5;241m-\u001b[39m(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/conv.py:974\u001b[0m, in \u001b[0;36mConvTranspose1d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    964\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    965\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    967\u001b[0m     output_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    972\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    973\u001b[0m )\n\u001b[0;32m--> 974\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given transposed=1, weight of size [48, 24, 15], expected input[1, 1536, 60390] to have 48 channels, but got 1536 channels instead"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Project Setup ===\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "track_name = \"Track00001\"\n",
    "track_path = os.path.join(project_root,\"ECE324_PROJECT\" ,\"data\", \"raw\", track_name)\n",
    "\n",
    "# === Dataset and Dataloader ===\n",
    "dataset = SingleTrackDataset(track_path)\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=100,\n",
    "    shuffle=False,\n",
    "    collate_fn=audio_pair_collate\n",
    ")\n",
    "\n",
    "# === Model Setup ===\n",
    "num_stems = len(dataset[0].target_waveforms)  # Infer number of output channels\n",
    "model = WaveUNet(input_channels=1, output_channels=num_stems).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# === Training Loop ===\n",
    "print(\"Training started...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        # AudioPair object: extract tensors\n",
    "        mixed = batch.mixed_waveform.to(device)\n",
    "        target = batch.target_waveforms.to(device)\n",
    "\n",
    "        # Train step\n",
    "        loss = train_step(mixed, target)\n",
    "        total_loss += loss\n",
    "\n",
    "        print(f\"Batch Loss: {loss:.6f}\")\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Total Loss: {total_loss:.6f}\\n\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
